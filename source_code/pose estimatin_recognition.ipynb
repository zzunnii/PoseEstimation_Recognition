{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5_A2RPgqy0V2",
    "outputId": "f703a9a3-5eba-44b4-eff3-6274d878986a"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (3540435477.py, line 515)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[6], line 515\u001b[1;36m\u001b[0m\n\u001b[1;33m    \"\"\"\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "포즈 감지 및 분류를 위한 딥러닝 모델 구현\n",
    "주요 기능: 실시간 포즈 감지, 다중 모델 앙상블, 포즈 분류\n",
    "작성자: 오승진\n",
    "마지막 수정: 2024\n",
    "\"\"\"\n",
    "\n",
    "# 기본 라이브러리 임포트\n",
    "import collections\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "from PIL import ImageFont, ImageDraw, Image\n",
    "from collections import deque\n",
    "import threading\n",
    "from queue import Queue\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# 딥러닝 관련 라이브러리\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from ultralytics import YOLO\n",
    "\n",
    "class PoseTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer 아키텍처 기반의 포즈 분류 모델\n",
    "    \n",
    "    주요 특징:\n",
    "    - 17개의 신체 키포인트를 처리하는 Transformer 기반 아키텍처\n",
    "    - 위치 임베딩을 통한 공간 정보 인코딩\n",
    "    - 멀티헤드 어텐션 메커니즘 활용\n",
    "    - 드롭아웃을 통한 과적합 방지\n",
    "    \n",
    "    Parameters:\n",
    "        input_dim (int): 입력 데이터의 차원 (기본값: 34, 17개 키포인트 x 2좌표)\n",
    "        num_classes (int): 분류할 포즈 클래스의 수 (기본값: 20)\n",
    "        num_heads (int): Transformer의 어텐션 헤드 수 (기본값: 8)\n",
    "        dim_feedforward (int): 피드포워드 네트워크의 은닉층 차원 (기본값: 512)\n",
    "        num_layers (int): Transformer 인코더 레이어의 수 (기본값: 4)\n",
    "    \n",
    "    입력 데이터 형식:\n",
    "        - (batch_size, 34) 크기의 텐서\n",
    "        - 각 키포인트는 (x, y) 좌표로 표현\n",
    "    \n",
    "    출력:\n",
    "        - (batch_size, num_classes) 크기의 텐서\n",
    "        - 각 클래스에 대한 확률 분포\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=34, num_classes=20, num_heads=8, dim_feedforward=512, num_layers=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 2D 좌표를 고차원 벡터로 투영하는 레이어\n",
    "        self.input_projection = nn.Linear(2, 128)\n",
    "        \n",
    "        # 17개 키포인트에 대한 위치 임베딩\n",
    "        # 학습 가능한 파라미터로, 각 키포인트의 공간적 위치 정보를 인코딩\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(17, 128))\n",
    "        \n",
    "        # Transformer 인코더 레이어 설정\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=128,          # 모델의 기본 차원\n",
    "            nhead=num_heads,      # 병렬 어텐션 헤드의 수\n",
    "            dim_feedforward=dim_feedforward,  # FFN의 은닉층 크기\n",
    "            dropout=0.1,          # 드롭아웃 비율\n",
    "            batch_first=True      # 배치 차원을 첫 번째로 설정\n",
    "        )\n",
    "        \n",
    "        # 여러 개의 인코더 레이어를 쌓아 Transformer 구성\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # 최종 분류를 위한 완전연결 레이어들\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 * 17, 512),  # 특징을 512차원으로 확장\n",
    "            nn.ReLU(),                 # 비선형성 추가\n",
    "            nn.Dropout(0.2),           # 과적합 방지\n",
    "            nn.Linear(512, 256),       # 차원 축소\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, num_classes)  # 최종 클래스 수만큼 출력\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        순전파 연산을 수행하는 메서드\n",
    "        \n",
    "        처리 단계:\n",
    "        1. 입력 텐서를 (배치크기, 17, 2) 형태로 재구성\n",
    "        2. 2D 좌표를 128차원 벡터로 투영\n",
    "        3. 위치 임베딩 정보 추가\n",
    "        4. Transformer를 통한 특징 추출\n",
    "        5. 분류기를 통한 최종 예측 생성\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): (배치크기, 34) 크기의 입력 텐서\n",
    "                            34 = 17개 키포인트 * 2(x,y 좌표)\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: (배치크기, num_classes) 크기의 클래스 예측값\n",
    "        \"\"\"\n",
    "        x = x.view(-1, 17, 2)        # 키포인트 좌표 형태로 재구성\n",
    "        x = self.input_projection(x)  # 고차원 특징 공간으로 투영\n",
    "        x = x + self.positional_embedding  # 위치 정보 추가\n",
    "        x = self.transformer(x)       # Transformer 처리\n",
    "        x = x.reshape(x.size(0), -1)  # 분류기 입력을 위해 평탄화\n",
    "        x = self.classifier(x)        # 최종 분류\n",
    "        return x\n",
    "    \n",
    "class PoseMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    다층 퍼셉트론(MLP) 기반의 포즈 분류 모델\n",
    "    \n",
    "    주요 특징:\n",
    "    - 잔차 연결(Residual connections)을 포함한 심층 신경망 구조\n",
    "    - BatchNormalization을 통한 학습 안정화\n",
    "    - 드롭아웃을 통한 과적합 방지\n",
    "    - 4개의 잔차 블록으로 구성된 깊은 아키텍처\n",
    "    \n",
    "    Parameters:\n",
    "        input_dim (int): 입력 데이터의 차원 (기본값: 34, 17개 키포인트 x 2좌표)\n",
    "        num_classes (int): 분류할 포즈 클래스의 수 (기본값: 20)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=34, num_classes=20):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 첫 번째 특징 추출 블록\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),    # 입력을 512차원으로 확장\n",
    "            nn.BatchNorm1d(512),          # 배치 정규화\n",
    "            nn.ReLU(),                    # 비선형성 추가\n",
    "            nn.Dropout(0.3)               # 30% 드롭아웃\n",
    "        )\n",
    "        \n",
    "        # 4개의 잔차 블록 생성\n",
    "        self.res_blocks = nn.ModuleList([\n",
    "            self._make_res_block(512, 512) for _ in range(4)\n",
    "        ])\n",
    "        \n",
    "        # 최종 분류기\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),          # 차원 축소\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_classes)    # 최종 클래스 예측\n",
    "        )\n",
    "\n",
    "    def _make_res_block(self, in_dim, out_dim):\n",
    "        \"\"\"\n",
    "        잔차 블록을 생성하는 헬퍼 메서드\n",
    "        \n",
    "        구조:\n",
    "        입력 -> Linear -> BatchNorm -> ReLU -> Dropout -> Linear -> BatchNorm -> (+) 입력 -> ReLU\n",
    "        \n",
    "        Args:\n",
    "            in_dim (int): 입력 차원\n",
    "            out_dim (int): 출력 차원\n",
    "            \n",
    "        Returns:\n",
    "            nn.Sequential: 구성된 잔차 블록\n",
    "        \"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.BatchNorm1d(out_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "            nn.BatchNorm1d(out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        순전파 연산을 수행하는 메서드\n",
    "        \n",
    "        처리 단계:\n",
    "        1. 첫 번째 특징 추출 블록 통과\n",
    "        2. 각 잔차 블록을 순차적으로 통과하며 잔차 연결 적용\n",
    "        3. 최종 분류기를 통한 예측 생성\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): (배치크기, input_dim) 크기의 입력 텐서\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: (배치크기, num_classes) 크기의 클래스 예측값\n",
    "        \"\"\"\n",
    "        x = self.block1(x)\n",
    "        for res_block in self.res_blocks:\n",
    "            identity = x                    # 잔차 연결을 위해 입력 저장\n",
    "            x = res_block(x)                # 잔차 블록 통과\n",
    "            x = F.relu(x + identity)        # 잔차 연결 및 활성화\n",
    "        x = self.classifier(x)              # 최종 분류\n",
    "        return x\n",
    "\n",
    "class PoseGRU(nn.Module):\n",
    "    \"\"\"\n",
    "    GRU(Gated Recurrent Unit) 기반의 포즈 분류 모델\n",
    "    \n",
    "    주요 특징:\n",
    "    - 양방향 GRU를 통한 시계열 데이터 처리\n",
    "    - 키포인트 간의 시퀀스적 관계 학습\n",
    "    - 공간적 임베딩을 통한 특징 추출\n",
    "    \n",
    "    Parameters:\n",
    "        input_dim (int): 입력 데이터의 차원 (기본값: 34)\n",
    "        num_classes (int): 분류할 포즈 클래스의 수 (기본값: 20)\n",
    "        hidden_dim (int): GRU의 은닉 상태 차원 (기본값: 256)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=34, num_classes=20, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 공간적 특징을 추출하는 임베딩 레이어\n",
    "        self.spatial_embedding = nn.Linear(2, hidden_dim)\n",
    "        \n",
    "        # 양방향 GRU 레이어\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=hidden_dim,         # 입력 크기\n",
    "            hidden_size=hidden_dim,        # 은닉 상태 크기\n",
    "            num_layers=3,                  # GRU 레이어 수\n",
    "            batch_first=True,              # 배치 차원을 첫 번째로 설정\n",
    "            dropout=0.2,                   # 레이어 간 드롭아웃\n",
    "            bidirectional=True             # 양방향 처리\n",
    "        )\n",
    "        \n",
    "        # 최종 분류기\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),  # 양방향이므로 *2\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        순전파 연산을 수행하는 메서드\n",
    "        \n",
    "        처리 단계:\n",
    "        1. 입력을 (배치크기, 17, 2) 형태로 재구성\n",
    "        2. 공간적 임베딩 적용\n",
    "        3. GRU를 통한 시퀀스 처리\n",
    "        4. 최종 시점의 은닉 상태를 사용하여 분류\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): (배치크기, input_dim) 크기의 입력 텐서\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: (배치크기, num_classes) 크기의 클래스 예측값\n",
    "        \"\"\"\n",
    "        x = x.view(-1, 17, 2)              # 키포인트 시퀀스 형태로 재구성\n",
    "        x = self.spatial_embedding(x)       # 공간적 특징 추출\n",
    "        x, _ = self.gru(x)                 # GRU 처리\n",
    "        x = x[:, -1, :]                    # 마지막 시점의 출력 사용\n",
    "        x = self.classifier(x)              # 최종 분류\n",
    "        return x\n",
    "    \n",
    "class PoseCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN(Convolutional Neural Network) 기반의 포즈 분류 모델\n",
    "    \n",
    "    주요 특징:\n",
    "    - 1D 컨볼루션을 사용한 키포인트 시퀀스 처리\n",
    "    - BatchNormalization을 통한 학습 안정화\n",
    "    - 계층적 특징 추출을 통한 공간적 관계 학습\n",
    "    \n",
    "    구조적 특징:\n",
    "    - 3개의 연속된 컨볼루션 레이어\n",
    "    - 채널 수를 점진적으로 증가 (64 -> 128 -> 256)\n",
    "    - 완전연결 레이어를 통한 최종 분류\n",
    "    \n",
    "    Parameters:\n",
    "        input_dim (int): 입력 데이터의 차원 (기본값: 34, 17개 키포인트 x 2좌표)\n",
    "        num_classes (int): 분류할 포즈 클래스의 수 (기본값: 20)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=34, num_classes=20):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 공간적 특징을 추출하는 1D 컨볼루션 레이어들\n",
    "        self.spatial_conv = nn.Sequential(\n",
    "            # 첫 번째 컨볼루션 블록: 2 -> 64 채널\n",
    "            nn.Conv1d(2, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # 두 번째 컨볼루션 블록: 64 -> 128 채널\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # 세 번째 컨볼루션 블록: 128 -> 256 채널\n",
    "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # 분류기 네트워크\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256 * 17, 512),  # 특징 맵을 평탄화하여 전결합층으로 전달\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),           # 과적합 방지\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        순전파 연산을 수행하는 메서드\n",
    "        \n",
    "        처리 단계:\n",
    "        1. 입력을 [batch, channels(x,y), keypoints] 형태로 재구성\n",
    "        2. 1D 컨볼루션을 통한 특징 추출\n",
    "        3. 추출된 특징을 평탄화하여 분류기에 전달\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): (배치크기, input_dim) 크기의 입력 텐서\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: (배치크기, num_classes) 크기의 클래스 예측값\n",
    "        \"\"\"\n",
    "        x = x.view(-1, 2, 17)         # [batch, channels(x,y), keypoints] 형태로 변환\n",
    "        x = self.spatial_conv(x)       # 컨볼루션 연산 적용\n",
    "        x = x.reshape(x.size(0), -1)   # 특징 맵 평탄화\n",
    "        x = self.classifier(x)         # 최종 분류\n",
    "        return x\n",
    "\n",
    "class EnhancedPoseViT(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer(ViT) 기반의 향상된 포즈 분류 모델\n",
    "    \n",
    "    주요 특징:\n",
    "    - 키포인트를 토큰으로 처리하는 Transformer 구조\n",
    "    - 글로벌 어텐션 풀링을 통한 특징 집계\n",
    "    - LayerNorm을 통한 정규화\n",
    "    - 어텐션 기반의 가중치 풀링\n",
    "    \n",
    "    아키텍처 특징:\n",
    "    - 초기 임베딩 레이어\n",
    "    - 멀티헤드 셀프 어텐션 기반 Transformer\n",
    "    - 어텐션 기반 풀링 메커니즘\n",
    "    - 계층적 분류기\n",
    "    \n",
    "    Parameters:\n",
    "        input_dim (int): 입력 데이터의 차원 (기본값: 34)\n",
    "        num_classes (int): 분류할 클래스 수 (기본값: 20)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=34, num_classes=20):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 초기 임베딩 레이어\n",
    "        self.embedding = nn.Linear(2, 128)\n",
    "        \n",
    "        # Transformer 인코더 설정\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=128,           # 모델 차원\n",
    "            nhead=4,               # 어텐션 헤드 수\n",
    "            dim_feedforward=256,   # 피드포워드 네트워크 차원\n",
    "            dropout=0.1,           # 드롭아웃 비율\n",
    "            batch_first=True       # 배치 우선 처리\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=3          # Transformer 레이어 수\n",
    "        )\n",
    "        \n",
    "        # 글로벌 어텐션 풀링\n",
    "        self.attention_pooling = nn.Sequential(\n",
    "            nn.Linear(128, 64),    # 차원 축소\n",
    "            nn.Tanh(),             # 활성화 함수\n",
    "            nn.Linear(64, 1)       # 어텐션 스코어 생성\n",
    "        )\n",
    "        \n",
    "        # 분류 헤드\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.LayerNorm(256),     # 레이어 정규화\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        순전파 연산을 수행하는 메서드\n",
    "        \n",
    "        처리 단계:\n",
    "        1. 입력을 키포인트 시퀀스로 재구성\n",
    "        2. 키포인트 임베딩\n",
    "        3. Transformer를 통한 특징 추출\n",
    "        4. 어텐션 기반 풀링으로 특징 집계\n",
    "        5. 최종 분류 수행\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): (배치크기, 34) 크기의 입력 텐서\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: (배치크기, num_classes) 크기의 클래스 예측값\n",
    "        \"\"\"\n",
    "        x = x.view(-1, 17, 2)          # 키포인트 시퀀스로 재구성\n",
    "        x = self.embedding(x)           # 임베딩 적용\n",
    "        x = self.transformer(x)         # Transformer 처리\n",
    "        \n",
    "        # 어텐션 가중치 계산 및 적용\n",
    "        attn_weights = self.attention_pooling(x)\n",
    "        attn_weights = F.softmax(attn_weights, dim=1)  # 소프트맥스로 정규화\n",
    "        x = (x * attn_weights).sum(dim=1)  # 가중치 적용 및 합산\n",
    "        \n",
    "        x = self.classifier(x)          # 최종 분류\n",
    "        return x\n",
    "    \n",
    "class StackingModel(nn.Module):\n",
    "    \"\"\"\n",
    "    앙상블 스태킹 기반의 포즈 분류 모델\n",
    "    \n",
    "    주요 특징:\n",
    "    - 5개의 기본 모델(Transformer, MLP, GRU, CNN, ViT)을 결합\n",
    "    - 각 모델의 예측에 대한 학습 가능한 가중치 적용\n",
    "    - 메타 학습을 통한 최적의 모델 조합 학습\n",
    "    \n",
    "    앙상블 구조:\n",
    "    1. 각 기본 모델이 독립적으로 예측 수행\n",
    "    2. 예측값들을 가중치와 함께 결합\n",
    "    3. 메타 특징 추출기를 통한 고차원 패턴 학습\n",
    "    4. 최종 분류기를 통한 예측\n",
    "    \n",
    "    Parameters:\n",
    "        num_classes (int): 분류할 클래스 수 (기본값: 20)\n",
    "        num_base_models (int): 기본 모델의 수 (기본값: 5)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=20, num_base_models=5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 기본 모델들 초기화\n",
    "        self.transformer = PoseTransformer()    # Transformer 기반 모델\n",
    "        self.mlp = PoseMLP()                   # MLP 기반 모델\n",
    "        self.gru = PoseGRU()                   # GRU 기반 모델\n",
    "        self.cnn = PoseCNN()                   # CNN 기반 모델\n",
    "        self.attention = EnhancedPoseViT()     # ViT 기반 모델\n",
    "        \n",
    "        # 메타 특징 추출기\n",
    "        # 각 모델의 예측을 결합하고 고차원 패턴을 학습\n",
    "        self.meta_features = nn.Sequential(\n",
    "            nn.Linear(num_classes * 5, 512),  # 모든 모델의 예측을 연결\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        # 최종 분류기\n",
    "        # 메타 특징을 기반으로 최종 예측 생성\n",
    "        self.final_classifier = nn.Sequential(\n",
    "            nn.Linear(256, num_classes),\n",
    "            nn.LogSoftmax(dim=1)  # 로그 소프트맥스 활성화\n",
    "        )\n",
    "        \n",
    "        # 각 모델의 가중치 파라미터\n",
    "        # 학습을 통해 각 모델의 중요도를 자동으로 결정\n",
    "        self.model_weights = nn.Parameter(torch.ones(5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        순전파 연산을 수행하는 메서드\n",
    "        \n",
    "        처리 단계:\n",
    "        1. 각 기본 모델에서 독립적으로 예측 수행\n",
    "        2. 예측값에 소프트맥스 적용하여 확률 분포 생성\n",
    "        3. 모델 가중치를 정규화하고 예측값에 적용\n",
    "        4. 가중치가 적용된 예측값들을 결합\n",
    "        5. 메타 특징 추출 및 최종 예측 생성\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): (배치크기, input_dim) 크기의 입력 텐서\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: (배치크기, num_classes) 크기의 최종 클래스 예측값\n",
    "        \"\"\"\n",
    "        # 각 모델의 예측 수행\n",
    "        trans_out = self.transformer(x)\n",
    "        mlp_out = self.mlp(x)\n",
    "        gru_out = self.gru(x)\n",
    "        cnn_out = self.cnn(x)\n",
    "        attn_out = self.attention(x)\n",
    "        \n",
    "        # 각 예측값을 확률 분포로 변환\n",
    "        trans_prob = F.softmax(trans_out, dim=1)\n",
    "        mlp_prob = F.softmax(mlp_out, dim=1)\n",
    "        gru_prob = F.softmax(gru_out, dim=1)\n",
    "        cnn_prob = F.softmax(cnn_out, dim=1)\n",
    "        attn_prob = F.softmax(attn_out, dim=1)\n",
    "        \n",
    "        # 모델 가중치 정규화\n",
    "        weights = F.softmax(self.model_weights, dim=0)\n",
    "        \n",
    "        # 가중치를 적용하여 예측값 결합\n",
    "        stacked_features = torch.cat([\n",
    "            trans_prob * weights[0],\n",
    "            mlp_prob * weights[1],\n",
    "            gru_prob * weights[2],\n",
    "            cnn_prob * weights[3],\n",
    "            attn_prob * weights[4]\n",
    "        ], dim=1)\n",
    "        \n",
    "        # 메타 특징 추출\n",
    "        meta_features = self.meta_features(stacked_features)\n",
    "        \n",
    "        # 최종 예측\n",
    "        output = self.final_classifier(meta_features)\n",
    "        return output\n",
    "\n",
    "# 클래스 매핑 정의\n",
    "class_mapping = {\n",
    "    \"\"\"\n",
    "    포즈 클래스 ID와 해당하는 한글 레이블 매핑\n",
    "    \n",
    "    각 숫자 키는 특정 포즈를 나타내며,\n",
    "    값은 해당 포즈의 한글 설명을 포함\n",
    "    \"\"\"\n",
    "    0: \"A포즈\",\n",
    "    1: \"I포즈\",\n",
    "    2: \"T포즈\",\n",
    "    3: \"계단 오르기\",\n",
    "    4: \"공을 던지려고 힘을 주는 자세\",\n",
    "    5: \"기지개\",\n",
    "    6: \"달리기(전력질주)\",\n",
    "    7: \"뒷짐\",\n",
    "    8: \"막대를 양손으로 잡고 골반 뒤쪽으로 쭉 뻗은 자세\",\n",
    "    9: \"머리 뒤 깍지를 낀 자세\",\n",
    "    10: \"몸을 앞으로 숙인 자세\",\n",
    "    11: \"발레\",\n",
    "    12: \"벽에 기대어 신발 신기\",\n",
    "    13: \"의자에 앉은 자세\",\n",
    "    14: \"조깅\",\n",
    "    15: \"통화하는 자세\",\n",
    "    16: \"팔짱\",\n",
    "    17: \"한 손과 반대편 발을 들며 신난 자세\",\n",
    "    18: \"한 다리 올리고 편하게 앉은 자세\",\n",
    "    19: \"허리 회전을 최대로 한 자세\"\n",
    "}\n",
    "\n",
    "def setup_device():\n",
    "    \"\"\"\n",
    "    GPU/CPU 사용 설정을 처리하는 함수\n",
    "    \n",
    "    주요 기능:\n",
    "    - CUDA GPU 사용 가능 여부 확인\n",
    "    - GPU 사용 가능시 CUDA 최적화 설정 적용\n",
    "    - GPU 메모리 및 디바이스 정보 출력\n",
    "    \n",
    "    Returns:\n",
    "        torch.device: 사용할 디바이스 객체\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        # CUDA 성능 최적화 설정\n",
    "        torch.backends.cudnn.benchmark = True      # 자동 벤치마킹으로 최적의 알고리즘 선택\n",
    "        torch.backends.cudnn.deterministic = False # 성능 향상을 위해 결정성 비활성화\n",
    "        \n",
    "        # GPU 정보 출력\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9  # GB 단위 변환\n",
    "        print(f\"Using GPU: {gpu_name}\")\n",
    "        print(f\"Total GPU Memory: {gpu_memory:.2f} GB\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print(\"GPU not available, using CPU\")\n",
    "    return device\n",
    "\n",
    "class PoseTransformer(nn.Module):\n",
    "    def __init__(self, input_dim=34, num_classes=20, num_heads=8, dim_feedforward=512, num_layers=4):\n",
    "        \"\"\"\n",
    "        포즈 분류를 위한 Transformer 모델 초기화\n",
    "        \n",
    "        주요 컴포넌트:\n",
    "        - MediaPipe 포즈 추정기\n",
    "        - YOLO 객체 탐지기\n",
    "        - 키포인트 처리 및 안정화 메커니즘\n",
    "        - 다양한 키포인트 매핑 및 시각화 설정\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.device = setup_device()  # GPU/CPU 설정\n",
    "        \n",
    "        # MediaPipe 포즈 추정기 초기화\n",
    "        self.mp_pose = mp.solutions.pose\n",
    "        self.pose = self.mp_pose.Pose(\n",
    "            static_image_mode=False,        # 비디오 스트림 모드\n",
    "            model_complexity=1,             # 중간 수준의 복잡도\n",
    "            smooth_landmarks=True,          # 키포인트 스무딩 활성화\n",
    "            min_detection_confidence=0.5,   # 최소 탐지 신뢰도\n",
    "            min_tracking_confidence=0.5     # 최소 추적 신뢰도\n",
    "        )\n",
    "\n",
    "        # YOLO 키포인트 인덱스 매핑 정의\n",
    "        self.yolo_keypoint_indices = {\n",
    "            'nose': 0,\n",
    "            'left_eye': 1,\n",
    "            'right_eye': 2,\n",
    "            'left_ear': 3,\n",
    "            'right_ear': 4,\n",
    "            # ... (기타 키포인트)\n",
    "        }\n",
    "\n",
    "        # 키포인트 열 매핑 정의 (정규화용)\n",
    "        self.column_mapping = {\n",
    "            'nose': ['Nose_x', 'Nose_y'],\n",
    "            'left_eye': ['Left_Eye_x', 'Left_Eye_y'],\n",
    "            # ... (기타 매핑)\n",
    "        }\n",
    "\n",
    "        # MediaPipe 키포인트 매핑\n",
    "        self.mediapipe_keypoints = {\n",
    "            self.mp_pose.PoseLandmark.NOSE: 'nose',\n",
    "            self.mp_pose.PoseLandmark.LEFT_EYE: 'left_eye',\n",
    "            # ... (기타 매핑)\n",
    "        }\n",
    "\n",
    "        # 스켈레톤 연결 정의\n",
    "        self.skeleton_connections = [\n",
    "            ('nose', 'left_eye'), ('nose', 'right_eye'),\n",
    "            ('left_eye', 'left_ear'), ('right_eye', 'right_ear'),\n",
    "            # ... (기타 연결)\n",
    "        ]\n",
    "\n",
    "        # 키포인트 색상 설정\n",
    "        self.keypoint_colors = {\n",
    "            # YOLO로 검출한 키포인트 (빨간색)\n",
    "            'nose': (0, 0, 255),\n",
    "            'left_eye': (0, 0, 255),\n",
    "            # MediaPipe로 검출한 키포인트 (파란색)\n",
    "            'left_shoulder': (255, 0, 0),\n",
    "            'right_shoulder': (255, 0, 0),\n",
    "            # ... (기타 색상)\n",
    "        }\n",
    "\n",
    "        # YOLO 모델 설정\n",
    "        self.yolo_pose = YOLO(yolo_pose_path)\n",
    "        if self.device.type == 'cuda':\n",
    "            self.yolo_pose.to(self.device)\n",
    "        self.yolo_pose.conf = 0.15  # 신뢰도 임계값\n",
    "        self.yolo_pose.iou = 0.35   # IOU 임계값\n",
    "\n",
    "        # 키포인트 안정화 설정\n",
    "        self.keypoint_buffer = collections.deque(maxlen=5)  # 최근 5프레임 저장\n",
    "        self.stability_threshold = 100       # 키포인트 안정성 임계값\n",
    "        self.movement_threshold = 0.5        # 움직임 감지 임계값\n",
    "        self.smoothing_factor = 0.7         # 스무딩 계수\n",
    "        self.previous_keypoints = None       # 이전 프레임 키포인트\n",
    "        self.lost_tracking_frames = 0        # 추적 실패 프레임 카운트\n",
    "        self.frame_width = 640              # 프레임 너비\n",
    "        self.frame_height = 480             # 프레임 높이\n",
    "\n",
    "        # 베이스 모델과 스태킹 모델 로드\n",
    "        self.base_models = self.load_base_models(model_paths)\n",
    "        self.stacking_model = self.load_stacking_model(model_paths['stacking'])\n",
    "\n",
    "    def _combination_keypoints(self, image):\n",
    "        \"\"\"\n",
    "        MediaPipe와 YOLO 모델의 키포인트를 결합하는 메서드\n",
    "        \n",
    "        주요 기능:\n",
    "        - 여러 사람의 키포인트 동시 처리\n",
    "        - YOLO와 MediaPipe의 장점을 결합한 하이브리드 접근\n",
    "        - 신뢰도 기반의 키포인트 선택\n",
    "        \n",
    "        처리 단계:\n",
    "        1. 이미지 전처리 및 변환\n",
    "        2. YOLO를 통한 키포인트 및 바운딩 박스 검출\n",
    "        3. MediaPipe를 통한 상세 키포인트 검출\n",
    "        4. 두 모델의 결과 결합\n",
    "        \n",
    "        Args:\n",
    "            image (np.ndarray): 처리할 BGR 이미지\n",
    "            \n",
    "        Returns:\n",
    "            list: 각 감지된 사람의 키포인트 정보를 담은 딕셔너리 리스트\n",
    "        \"\"\"\n",
    "        try:\n",
    "            height, width = image.shape[:2]\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            all_person_keypoints = []  # 여러 사람의 키포인트 저장용\n",
    "\n",
    "            # YOLO 포즈 검출 수행\n",
    "            yolo_results = self.yolo_pose(image_rgb)\n",
    "            person_boxes = self.yolo(image_rgb)\n",
    "            \n",
    "            if len(yolo_results) > 0:\n",
    "                # 각 감지된 사람에 대한 처리\n",
    "                for person_idx in range(len(yolo_results[0].keypoints)):\n",
    "                    combination_keypoints = {}\n",
    "                    \n",
    "                    # YOLO 키포인트 처리\n",
    "                    if hasattr(yolo_results[0], 'keypoints'):\n",
    "                        kpts = yolo_results[0].keypoints[person_idx]\n",
    "                        if hasattr(kpts, 'data') and len(kpts.data) > 0:\n",
    "                            yolo_kpts = kpts.data.cpu().numpy()[0]\n",
    "                            for idx, (key, _) in enumerate(self.yolo_keypoint_indices.items()):\n",
    "                                if idx < len(yolo_kpts):\n",
    "                                    x, y, conf = yolo_kpts[idx]\n",
    "                                    if conf > 0.2:  # 신뢰도 임계값\n",
    "                                        combination_keypoints[key] = [int(x), int(y)]\n",
    "\n",
    "                    # MediaPipe 처리\n",
    "                    mp_results = self.pose.process(image_rgb)\n",
    "                    if mp_results.pose_landmarks:\n",
    "                        landmarks = mp_results.pose_landmarks.landmark\n",
    "                        for landmark_id, name in self.mediapipe_keypoints.items():\n",
    "                            landmark = landmarks[landmark_id.value]\n",
    "                            if landmark.visibility > 0.3:  # 가시성 임계값\n",
    "                                x = int(landmark.x * width)\n",
    "                                y = int(landmark.y * height)\n",
    "                                # 상체 키포인트는 MediaPipe 우선\n",
    "                                if name in ['left_shoulder', 'right_shoulder',\n",
    "                                        'left_elbow', 'right_elbow',\n",
    "                                        'left_wrist', 'right_wrist']:\n",
    "                                    combination_keypoints[name] = [x, y]\n",
    "\n",
    "                    # 바운딩 박스 정보 추가\n",
    "                    if len(person_boxes) > person_idx:\n",
    "                        box = person_boxes[0].boxes.data[person_idx]\n",
    "                        x1, y1, x2, y2, conf, _ = box.cpu().numpy()\n",
    "                        combination_keypoints['bbox'] = [\n",
    "                            int(x1), int(y1), int(x2), int(y2), float(conf)\n",
    "                        ]\n",
    "\n",
    "                    # 유효성 검증 및 추가\n",
    "                    if len(combination_keypoints) > 5:  # 최소 키포인트 수 검증\n",
    "                        all_person_keypoints.append(combination_keypoints)\n",
    "\n",
    "            return all_person_keypoints\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"키포인트 결합 중 오류 발생: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def smooth_keypoints(self, current_keypoints):\n",
    "        \"\"\"\n",
    "        키포인트 움직임을 부드럽게 하는 스무딩 처리 메서드\n",
    "        \n",
    "        주요 기능:\n",
    "        - 시간적 필터링을 통한 키포인트 안정화\n",
    "        - 지수 이동 평균을 사용한 스무딩\n",
    "        - 급격한 변화 감지 및 보정\n",
    "        \n",
    "        처리 단계:\n",
    "        1. 이전 프레임들의 유효한 키포인트 확인\n",
    "        2. 지수 이동 평균 계산\n",
    "        3. 스무딩된 좌표 계산\n",
    "        \n",
    "        Args:\n",
    "            current_keypoints (dict): 현재 프레임의 키포인트\n",
    "            \n",
    "        Returns:\n",
    "            dict: 스무딩된 키포인트\n",
    "        \"\"\"\n",
    "        if not self.keypoint_buffer:\n",
    "            return current_keypoints\n",
    "\n",
    "        smoothed_keypoints = {}\n",
    "        for key in current_keypoints:\n",
    "            if current_keypoints[key] == [0, 0]:\n",
    "                continue\n",
    "\n",
    "            # 버퍼에서 유효한 이전 키포인트 추출\n",
    "            valid_points = [\n",
    "                buff[key] for buff in self.keypoint_buffer\n",
    "                if key in buff and buff[key] != [0, 0]\n",
    "            ]\n",
    "\n",
    "            if valid_points:\n",
    "                # 지수 이동 평균 계산\n",
    "                curr_x, curr_y = current_keypoints[key]\n",
    "                prev_x, prev_y = valid_points[-1]\n",
    "\n",
    "                # 스무딩 적용\n",
    "                smoothed_x = self.smoothing_factor * curr_x + (1 - self.smoothing_factor) * prev_x\n",
    "                smoothed_y = self.smoothing_factor * curr_y + (1 - self.smoothing_factor) * prev_y\n",
    "\n",
    "                smoothed_keypoints[key] = [int(smoothed_x), int(smoothed_y)]\n",
    "            else:\n",
    "                smoothed_keypoints[key] = current_keypoints[key]\n",
    "\n",
    "        return smoothed_keypoints\n",
    "    \n",
    "    def extract_keypoints(self, image):\n",
    "        \"\"\"\n",
    "        이미지에서 키포인트를 추출하는 메인 메서드\n",
    "        \n",
    "        주요 기능:\n",
    "        - MediaPipe와 YOLO 모델을 통합적으로 사용\n",
    "        - 해부학적 제약 조건 적용\n",
    "        - 누락된 키포인트 추정\n",
    "        - 키포인트 안정성 검증\n",
    "        \n",
    "        처리 과정:\n",
    "        1. 입력 이미지 유효성 검사\n",
    "        2. 모델 기반 키포인트 추출\n",
    "        3. 해부학적 제약 조건 적용\n",
    "        4. 누락된 키포인트 보간\n",
    "        \n",
    "        Args:\n",
    "            image (np.ndarray): 처리할 입력 이미지\n",
    "            \n",
    "        Returns:\n",
    "            dict: 처리된 키포인트 정보 또는 None (실패 시)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not isinstance(image, np.ndarray) or image.size == 0:\n",
    "                return None\n",
    "\n",
    "            # 키포인트 추출 및 결합\n",
    "            keypoints = self._combination_keypoints(image)\n",
    "            if keypoints is None:\n",
    "                return None\n",
    "\n",
    "            # 해부학적 제약 조건 적용\n",
    "            keypoints = self._apply_anatomical_constraints(keypoints, image.shape[0], image.shape[1])\n",
    "\n",
    "            # 누락된 키포인트 추정\n",
    "            keypoints = self.estimate_hidden_keypoints(keypoints)\n",
    "\n",
    "            return keypoints\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"키포인트 추출 중 오류 발생: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _apply_anatomical_constraints(self, keypoints, height, width):\n",
    "        \"\"\"\n",
    "        해부학적 제약 조건을 적용하여 키포인트의 유효성을 검증하고 보정하는 메서드\n",
    "        \n",
    "        주요 기능:\n",
    "        - 얼굴 부위 키포인트 제약 적용\n",
    "        - 다리 관절 체인 검증\n",
    "        - 비현실적인 관절 위치 보정\n",
    "        \n",
    "        제약 조건:\n",
    "        1. 얼굴 키포인트 간 최대 거리 제한\n",
    "        2. 관절 간 최대 거리 제한\n",
    "        3. 해부학적으로 가능한 관절 각도 범위 검증\n",
    "        \n",
    "        Args:\n",
    "            keypoints (dict): 입력 키포인트\n",
    "            height (int): 이미지 높이\n",
    "            width (int): 이미지 너비\n",
    "            \n",
    "        Returns:\n",
    "            dict: 제약 조건이 적용된 키포인트\n",
    "        \"\"\"\n",
    "        constrained_points = keypoints.copy()\n",
    "\n",
    "        # 얼굴 키포인트 제약\n",
    "        if 'nose' in constrained_points:\n",
    "            nose_x, nose_y = constrained_points['nose']\n",
    "            face_radius = width * 0.1  # 얼굴 크기 제한\n",
    "\n",
    "            # 얼굴 부위 키포인트 위치 제한\n",
    "            for name in ['left_eye', 'right_eye', 'left_ear', 'right_ear']:\n",
    "                if name in constrained_points:\n",
    "                    x, y = constrained_points[name]\n",
    "                    dist = np.sqrt((x - nose_x) ** 2 + (y - nose_y) ** 2)\n",
    "\n",
    "                    if dist > face_radius:\n",
    "                        # 최대 거리 제한 적용\n",
    "                        angle = np.arctan2(y - nose_y, x - nose_x)\n",
    "                        constrained_points[name] = [\n",
    "                            int(nose_x + face_radius * np.cos(angle)),\n",
    "                            int(nose_y + face_radius * np.sin(angle))\n",
    "                        ]\n",
    "\n",
    "        # 다리 키포인트 제약\n",
    "        for side in ['left', 'right']:\n",
    "            hip_name = f'{side}_hip'\n",
    "            knee_name = f'{side}_knee'\n",
    "            ankle_name = f'{side}_ankle'\n",
    "\n",
    "            if all(name in constrained_points for name in [hip_name, knee_name, ankle_name]):\n",
    "                hip = constrained_points[hip_name]\n",
    "                knee = constrained_points[knee_name]\n",
    "                ankle = constrained_points[ankle_name]\n",
    "\n",
    "                # 무릎-엉덩이 거리 제한\n",
    "                knee_hip_dist = np.sqrt((knee[0] - hip[0]) ** 2 + (knee[1] - hip[1]) ** 2)\n",
    "                max_knee_dist = height * 0.3\n",
    "\n",
    "                if knee_hip_dist > max_knee_dist:\n",
    "                    angle = np.arctan2(knee[1] - hip[1], knee[0] - hip[0])\n",
    "                    constrained_points[knee_name] = [\n",
    "                        int(hip[0] + max_knee_dist * np.cos(angle)),\n",
    "                        int(hip[1] + max_knee_dist * np.sin(angle))\n",
    "                    ]\n",
    "\n",
    "                # 발목-무릎 거리 제한\n",
    "                ankle_knee_dist = np.sqrt((ankle[0] - knee[0]) ** 2 + (ankle[1] - knee[1]) ** 2)\n",
    "                max_ankle_dist = height * 0.3\n",
    "\n",
    "                if ankle_knee_dist > max_ankle_dist:\n",
    "                    angle = np.arctan2(ankle[1] - knee[1], ankle[0] - knee[0])\n",
    "                    constrained_points[ankle_name] = [\n",
    "                        int(knee[0] + max_ankle_dist * np.cos(angle)),\n",
    "                        int(knee[1] + max_ankle_dist * np.sin(angle))\n",
    "                    ]\n",
    "\n",
    "        return constrained_points\n",
    "    \n",
    "    def estimate_hidden_keypoints(self, keypoints):\n",
    "        \"\"\"\n",
    "        숨겨지거나 누락된 키포인트를 추정하는 메서드\n",
    "        \n",
    "        주요 기능:\n",
    "        - 이전 프레임 정보를 활용한 키포인트 추정\n",
    "        - 움직임 기반 예측 및 보정\n",
    "        - 급격한 움직임 감지 및 처리\n",
    "        - 시간적 일관성 유지\n",
    "        \n",
    "        처리 단계:\n",
    "        1. 현재 키포인트 유효성 검사\n",
    "        2. 이전 프레임 데이터 활용\n",
    "        3. 움직임 패턴 분석\n",
    "        4. 누락된 키포인트 추정\n",
    "        \n",
    "        Args:\n",
    "            keypoints (dict): 현재 프레임의 키포인트 데이터\n",
    "            \n",
    "        Returns:\n",
    "            dict: 추정된 키포인트가 포함된 완성된 데이터\n",
    "        \"\"\"\n",
    "        if keypoints is None:\n",
    "            return self.previous_keypoints\n",
    "\n",
    "        estimated_keypoints = keypoints.copy()\n",
    "\n",
    "        # 움직임이 큰 경우의 처리\n",
    "        if self.previous_keypoints:\n",
    "            for key in keypoints:\n",
    "                curr_x, curr_y = keypoints[key]\n",
    "                if key in self.previous_keypoints:\n",
    "                    prev_x, prev_y = self.previous_keypoints[key]\n",
    "\n",
    "                    # 현재 위치가 유효하지 않은 경우 처리\n",
    "                    if (curr_x == 0 and curr_y == 0) or \\\n",
    "                            curr_x < 0 or curr_x > self.frame_width or \\\n",
    "                            curr_y < 0 or curr_y > self.frame_height:\n",
    "\n",
    "                        # 이전 프레임들의 데이터를 활용한 추정\n",
    "                        if len(self.keypoint_buffer) >= 2:\n",
    "                            older_x, older_y = self.keypoint_buffer[-2].get(key, (prev_x, prev_y))\n",
    "                            if older_x != 0 and older_y != 0:\n",
    "                                # 이동 방향과 속도를 고려한 예측\n",
    "                                dx = prev_x - older_x\n",
    "                                dy = prev_y - older_y\n",
    "                                estimated_x = int(prev_x + dx * self.movement_threshold)\n",
    "                                estimated_y = int(prev_y + dy * self.movement_threshold)\n",
    "                                \n",
    "                                # 예측값이 화면 범위 내에 있는지 확인\n",
    "                                estimated_x = max(0, min(estimated_x, self.frame_width))\n",
    "                                estimated_y = max(0, min(estimated_y, self.frame_height))\n",
    "                                \n",
    "                                estimated_keypoints[key] = [estimated_x, estimated_y]\n",
    "                                continue\n",
    "\n",
    "                            estimated_keypoints[key] = self.previous_keypoints[key]\n",
    "                    else:\n",
    "                        # 급격한 움직임 감지 및 보정\n",
    "                        distance = np.sqrt((curr_x - prev_x) ** 2 + (curr_y - prev_y) ** 2)\n",
    "                        if distance > self.stability_threshold:\n",
    "                            if len(self.keypoint_buffer) > 0:\n",
    "                                valid_points = [\n",
    "                                    buff[key] for buff in self.keypoint_buffer\n",
    "                                    if key in buff and buff[key] != [0, 0]\n",
    "                                ]\n",
    "                                if valid_points:\n",
    "                                    recent_points = valid_points[-2:]\n",
    "                                    if len(recent_points) >= 2:\n",
    "                                        dx = recent_points[1][0] - recent_points[0][0]\n",
    "                                        dy = recent_points[1][1] - recent_points[0][1]\n",
    "                                        estimated_keypoints[key] = [\n",
    "                                            int(recent_points[1][0] + dx * 0.5),\n",
    "                                            int(recent_points[1][1] + dy * 0.5)\n",
    "                                        ]\n",
    "                                    else:\n",
    "                                        estimated_keypoints[key] = valid_points[-1]\n",
    "\n",
    "        # 버퍼 업데이트\n",
    "        self.keypoint_buffer.append(estimated_keypoints)\n",
    "        self.previous_keypoints = estimated_keypoints\n",
    "\n",
    "        return estimated_keypoints\n",
    "\n",
    "    def _validate_leg_chain(self, keypoints, side):\n",
    "        \"\"\"\n",
    "        다리 관절 체인의 유효성을 검증하는 메서드\n",
    "        \n",
    "        주요 기능:\n",
    "        - 다리 관절(엉덩이-무릎-발목) 간의 거리 검증\n",
    "        - 비현실적인 관절 배치 감지\n",
    "        - 이전 프레임 데이터를 활용한 보정\n",
    "        \n",
    "        검증 항목:\n",
    "        1. 관절 간 거리의 현실성\n",
    "        2. 전체 다리 길이의 적절성\n",
    "        3. 관절 각도의 유효성\n",
    "        \n",
    "        Args:\n",
    "            keypoints (dict): 검증할 키포인트 데이터\n",
    "            side (str): 검증할 다리 방향 ('left' 또는 'right')\n",
    "        \"\"\"\n",
    "        hip = np.array(keypoints[f'{side}_hip'])\n",
    "        knee = np.array(keypoints[f'{side}_knee'])\n",
    "        ankle = np.array(keypoints[f'{side}_ankle'])\n",
    "\n",
    "        # 관절 간 거리 계산 및 검증\n",
    "        hip_knee_dist = np.linalg.norm(hip - knee)\n",
    "        knee_ankle_dist = np.linalg.norm(knee - ankle)\n",
    "        hip_ankle_dist = np.linalg.norm(hip - ankle)\n",
    "\n",
    "        # 비현실적인 관절 배치 검사 (전체 길이가 각 부분의 합보다 20% 이상 큰 경우)\n",
    "        if hip_ankle_dist > (hip_knee_dist + knee_ankle_dist) * 1.2:\n",
    "            if self.previous_keypoints:\n",
    "                for joint in [f'{side}_knee', f'{side}_ankle']:\n",
    "                    if joint in self.previous_keypoints:\n",
    "                        keypoints[joint] = self.previous_keypoints[joint]\n",
    "\n",
    "    def normalize_keypoints(self, keypoints):\n",
    "        \"\"\"\n",
    "        포즈 키포인트를 정규화하는 메서드\n",
    "        \n",
    "        주요 기능:\n",
    "        - 키포인트 좌표의 정규화 처리\n",
    "        - hip center 기준 상대 위치 계산\n",
    "        - 어깨 너비 기반 스케일 정규화\n",
    "        - 누락된 데이터 처리\n",
    "        \n",
    "        정규화 과정:\n",
    "        1. 데이터 유효성 검증\n",
    "        2. hip center 계산 (없을 경우 어깨 중심 사용)\n",
    "        3. 상대 좌표 변환\n",
    "        4. 스케일 정규화\n",
    "        \n",
    "        Args:\n",
    "            keypoints (dict): 원본 키포인트 데이터\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: 정규화된 키포인트 데이터프레임 또는 None (실패 시)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 데이터프레임용 평탄화된 리스트 생성\n",
    "            kp_flat = []\n",
    "            all_zeros = True  # 모든 값이 0인지 체크\n",
    "\n",
    "            # 키포인트 좌표 추출 및 검증\n",
    "            for name, cols in self.column_mapping.items():\n",
    "                if name in keypoints:\n",
    "                    x, y = keypoints[name]\n",
    "                    if x != 0 or y != 0:\n",
    "                        all_zeros = False\n",
    "                    kp_flat.extend([x, y])\n",
    "                else:\n",
    "                    kp_flat.extend([0, 0])\n",
    "\n",
    "            if all_zeros:\n",
    "                print(\"경고: 모든 키포인트가 0입니다.\")\n",
    "                return None\n",
    "\n",
    "            # 데이터프레임 생성\n",
    "            columns = [col for pair in self.column_mapping.values() for col in pair]\n",
    "            df = pd.DataFrame([kp_flat], columns=columns)\n",
    "\n",
    "            # hip center 계산을 위한 검증\n",
    "            right_hip_x = df['Right_Hip_x'].iloc[0]\n",
    "            left_hip_x = df['Left_Hip_x'].iloc[0]\n",
    "            right_hip_y = df['Right_Hip_y'].iloc[0]\n",
    "            left_hip_y = df['Left_Hip_y'].iloc[0]\n",
    "\n",
    "            # hip center 계산 (hip이 없는 경우 어깨 중심 사용)\n",
    "            if (right_hip_x == 0 and left_hip_x == 0) or (right_hip_y == 0 and left_hip_y == 0):\n",
    "                hip_center_x = (df['Right_Shoulder_x'].iloc[0] + df['Left_Shoulder_x'].iloc[0]) / 2\n",
    "                hip_center_y = (df['Right_Shoulder_y'].iloc[0] + df['Left_Shoulder_y'].iloc[0]) / 2\n",
    "            else:\n",
    "                hip_center_x = (right_hip_x + left_hip_x) / 2\n",
    "                hip_center_y = (right_hip_y + left_hip_y) / 2\n",
    "\n",
    "            # hip center 기준 정규화\n",
    "            for col in df.columns:\n",
    "                if col.endswith('_x') and df[col].iloc[0] != 0:\n",
    "                    df[col] = df[col] - hip_center_x\n",
    "                elif col.endswith('_y') and df[col].iloc[0] != 0:\n",
    "                    df[col] = df[col] - hip_center_y\n",
    "\n",
    "            # 어깨 너비 기준 스케일 정규화\n",
    "            shoulder_width = np.sqrt(\n",
    "                (df['Right_Shoulder_x'] - df['Left_Shoulder_x']) ** 2 +\n",
    "                (df['Right_Shoulder_y'] - df['Left_Shoulder_y']) ** 2\n",
    "            )\n",
    "\n",
    "            if shoulder_width.iloc[0] > 0:\n",
    "                for col in df.columns:\n",
    "                    if (col.endswith('_x') or col.endswith('_y')) and df[col].iloc[0] != 0:\n",
    "                        df[col] = df[col] / shoulder_width.iloc[0]\n",
    "\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"키포인트 정규화 중 오류 발생: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def predict(self, image):\n",
    "        \"\"\"\n",
    "        입력 이미지에서 포즈를 예측하는 메인 메서드\n",
    "        \n",
    "        주요 기능:\n",
    "        - 키포인트 추출\n",
    "        - 데이터 정규화\n",
    "        - 앙상블 모델을 통한 예측\n",
    "        - 클래스 매핑 및 결과 반환\n",
    "        \n",
    "        처리 단계:\n",
    "        1. 이미지에서 키포인트 추출\n",
    "        2. 키포인트 정규화\n",
    "        3. 모델을 통한 예측 수행\n",
    "        4. 예측 결과를 클래스 레이블로 변환\n",
    "        \n",
    "        Args:\n",
    "            image (np.ndarray): 입력 이미지\n",
    "            \n",
    "        Returns:\n",
    "            str: 예측된 포즈 클래스명 또는 오류 메시지\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 키포인트 추출\n",
    "            keypoints = self.extract_keypoints(image)\n",
    "            if keypoints is None:\n",
    "                return \"포즈가 감지되지 않음\"\n",
    "\n",
    "            # 키포인트 정규화\n",
    "            normalized_kps = self.normalize_keypoints(keypoints)\n",
    "            if normalized_kps is None:\n",
    "                return \"키포인트 정규화 실패\"\n",
    "\n",
    "            # 모델 입력을 위한 텐서 변환\n",
    "            inputs = torch.FloatTensor(normalized_kps.values).to(self.device)\n",
    "\n",
    "            # 각 기본 모델의 예측 수행\n",
    "            base_predictions = []\n",
    "            with torch.no_grad():\n",
    "                for model in self.base_models.values():\n",
    "                    outputs = model(inputs)\n",
    "                    probs = F.softmax(outputs, dim=1)\n",
    "                    base_predictions.append(probs)\n",
    "\n",
    "                # 스태킹 모델을 통한 최종 예측\n",
    "                stacked_output = self.stacking_model(inputs)\n",
    "                final_probs = F.softmax(stacked_output, dim=1)\n",
    "                predicted_class = final_probs.argmax(dim=1).item()\n",
    "\n",
    "            return class_mapping[predicted_class]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"예측 중 오류 발생: {str(e)}\")\n",
    "            return \"예측 실패\"\n",
    "        \n",
    "    def load_base_models(self, model_paths):\n",
    "        \"\"\"\n",
    "        기본 포즈 분류 모델들을 로드하는 메서드\n",
    "        \n",
    "        주요 기능:\n",
    "        - 다양한 아키텍처의 모델 로드 (Transformer, MLP, GRU, CNN, ViT)\n",
    "        - 체크포인트에서 모델 상태 복원\n",
    "        - GPU/CPU 디바이스 할당\n",
    "        - 모델을 평가 모드로 설정\n",
    "        \n",
    "        모델 초기화 과정:\n",
    "        1. 체크포인트 파일 로드\n",
    "        2. 모델 아키텍처별 인스턴스 생성\n",
    "        3. 가중치 로드 및 디바이스 할당\n",
    "        4. 평가 모드 설정\n",
    "        \n",
    "        Args:\n",
    "            model_paths (dict): 모델별 체크포인트 파일 경로\n",
    "            \n",
    "        Returns:\n",
    "            dict: 초기화된 모델들의 딕셔너리\n",
    "        \"\"\"\n",
    "        models = {}\n",
    "        for model_name, path in model_paths.items():\n",
    "            if model_name == 'stacking':  # 스태킹 모델은 별도 처리\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # 체크포인트 로드\n",
    "                checkpoint = torch.load(path, map_location=self.device)\n",
    "                state_dict = checkpoint['model_state_dict']\n",
    "                \n",
    "                # 모델 아키텍처별 인스턴스 생성\n",
    "                if model_name == 'transformer':\n",
    "                    model = PoseTransformer(input_dim=34, num_classes=20)\n",
    "                elif model_name == 'mlp':\n",
    "                    model = PoseMLP()\n",
    "                elif model_name == 'gru':\n",
    "                    model = PoseGRU()\n",
    "                elif model_name == 'cnn':\n",
    "                    model = PoseCNN()\n",
    "                else:  # vit\n",
    "                    model = EnhancedPoseViT()\n",
    "\n",
    "                # 가중치 로드 및 모델 설정\n",
    "                model.load_state_dict(state_dict, strict=True)\n",
    "                model = model.to(self.device)  # GPU/CPU 할당\n",
    "                model.eval()  # 평가 모드 설정\n",
    "                models[model_name] = model\n",
    "                print(f\"{model_name} 모델 로드 성공\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"{model_name} 모델 로드 중 오류 발생: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        return models\n",
    "\n",
    "    def load_stacking_model(self, path):\n",
    "        \"\"\"\n",
    "        앙상블 스태킹 모델을 로드하는 메서드\n",
    "        \n",
    "        주요 기능:\n",
    "        - 스태킹 모델 인스턴스 생성\n",
    "        - 체크포인트에서 가중치 복원\n",
    "        - GPU/CPU 디바이스 할당\n",
    "        - 평가 모드 설정\n",
    "        \n",
    "        Args:\n",
    "            path (str): 스태킹 모델 체크포인트 파일 경로\n",
    "            \n",
    "        Returns:\n",
    "            StackingModel: 초기화된 스태킹 모델 또는 None (실패 시)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 스태킹 모델 인스턴스 생성\n",
    "            model = StackingModel()\n",
    "            \n",
    "            # 체크포인트 로드 및 가중치 복원\n",
    "            checkpoint = torch.load(path, map_location=self.device)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "            \n",
    "            # 디바이스 할당 및 평가 모드 설정\n",
    "            model = model.to(self.device)\n",
    "            model.eval()\n",
    "            return model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"스태킹 모델 로드 중 오류 발생: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "class RealtimePosePredictor:\n",
    "    \"\"\"\n",
    "    실시간 포즈 예측을 위한 클래스\n",
    "    \n",
    "    주요 기능:\n",
    "    - 실시간 비디오 스트림 처리\n",
    "    - 멀티스레딩을 통한 성능 최적화\n",
    "    - CPU 최적화된 처리 파이프라인\n",
    "    - 한글 텍스트 렌더링\n",
    "    \n",
    "    구성 요소:\n",
    "    - YOLO 객체 탐지기\n",
    "    - 포즈 예측 모델\n",
    "    - 프레임 처리 큐\n",
    "    - FPS 측정 시스템\n",
    "    \"\"\"\n",
    "    def __init__(self, model_paths, yolo_path, yolo_pose_path, font_path):\n",
    "        \"\"\"\n",
    "        실시간 포즈 예측기 초기화\n",
    "        \n",
    "        Args:\n",
    "            model_paths (dict): 각 모델의 체크포인트 파일 경로\n",
    "            yolo_path (str): YOLO 모델 파일 경로\n",
    "            yolo_pose_path (str): YOLO 포즈 모델 파일 경로\n",
    "            font_path (str): 한글 폰트 파일 경로\n",
    "            \n",
    "        초기화 과정:\n",
    "        1. CPU 최적화 설정\n",
    "        2. YOLO 모델 초기화\n",
    "        3. 멀티스레딩 설정\n",
    "        4. 성능 최적화를 위한 큐 설정\n",
    "        \"\"\"\n",
    "        self.device = torch.device('cpu')\n",
    "        print(\"CPU 모드로 실행중...\")\n",
    "        \n",
    "        # 경로 저장\n",
    "        self.yolo_path = yolo_path\n",
    "        self.yolo_pose_path = yolo_pose_path\n",
    "        self.font_path = font_path\n",
    "        \n",
    "        # YOLO 모델 초기화 (CPU 최적화)\n",
    "        try:\n",
    "            self.yolo = YOLO(self.yolo_path)\n",
    "            self.yolo_pose = YOLO(self.yolo_pose_path)\n",
    "            \n",
    "            # CPU 최적화 설정\n",
    "            self.yolo.conf = 0.3        # 신뢰도 임계값 조정\n",
    "            self.yolo_pose.conf = 0.3\n",
    "            self.yolo.iou = 0.45       # IOU 임계값 조정\n",
    "            self.yolo_pose.iou = 0.45\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"YOLO 모델 초기화 중 오류 발생: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _process_frames(self):\n",
    "        \"\"\"\n",
    "        프레임 처리를 위한 백그라운드 스레드 메서드\n",
    "        \n",
    "        주요 기능:\n",
    "        - 프레임 큐에서 이미지를 가져와 처리\n",
    "        - CPU 리소스 최적화\n",
    "        - 연속적인 프레임 처리\n",
    "        - 처리된 결과를 결과 큐에 저장\n",
    "        \n",
    "        처리 과정:\n",
    "        1. 프레임 큐에서 이미지 추출\n",
    "        2. 이미지 크기 조정\n",
    "        3. 단일 프레임 처리 실행\n",
    "        4. 결과 큐에 처리된 프레임 저장\n",
    "        \"\"\"\n",
    "        while self.running:\n",
    "            try:\n",
    "                if not self.frame_queue.empty():\n",
    "                    frame = self.frame_queue.get()\n",
    "                    # 프레임 크기 조정으로 처리 속도 향상\n",
    "                    frame = cv2.resize(frame, (self.frame_width, self.frame_height))\n",
    "                    result = self._process_single_frame(frame)\n",
    "                    if not self.result_queue.full():\n",
    "                        self.result_queue.put(result)\n",
    "                else:\n",
    "                    # CPU 부하 감소를 위한 짧은 대기\n",
    "                    time.sleep(0.01)\n",
    "            except Exception as e:\n",
    "                print(f\"프레임 처리 중 오류 발생: {e}\")\n",
    "                continue\n",
    "\n",
    "    def _process_single_frame(self, frame):\n",
    "        \"\"\"\n",
    "        단일 프레임 처리를 위한 메서드\n",
    "        \n",
    "        주요 기능:\n",
    "        - 여러 사람의 포즈 동시 감지\n",
    "        - 키포인트 추출 및 시각화\n",
    "        - 바운딩 박스 및 스켈레톤 그리기\n",
    "        - FPS 정보 표시\n",
    "        \n",
    "        처리 단계:\n",
    "        1. 입력 프레임 검증\n",
    "        2. 포즈 감지 및 키포인트 추출\n",
    "        3. 시각적 요소 추가 (스켈레톤, 바운딩 박스 등)\n",
    "        4. FPS 계산 및 표시\n",
    "        \n",
    "        Args:\n",
    "            frame (np.ndarray): 처리할 입력 프레임\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: 처리된 프레임\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if frame is None or not isinstance(frame, np.ndarray):\n",
    "                return frame\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # 여러 사람의 키포인트 추출\n",
    "            all_keypoints = self.pose_predictor.extract_keypoints(frame)\n",
    "            \n",
    "            if all_keypoints:\n",
    "                for person_keypoints in all_keypoints:\n",
    "                    # 바운딩 박스 그리기\n",
    "                    if 'bbox' in person_keypoints:\n",
    "                        x1, y1, x2, y2, conf = person_keypoints['bbox']\n",
    "                        cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)),\n",
    "                                    (0, 255, 0), 2)\n",
    "                        cv2.putText(frame, f'Person {conf:.2f}', (int(x1), int(y1) - 10),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "                    try:\n",
    "                        # 키포인트 정규화 및 포즈 예측\n",
    "                        normalized_kps = self.pose_predictor.normalize_keypoints(person_keypoints)\n",
    "                        if normalized_kps is not None:\n",
    "                            pose_prediction = self.pose_predictor.predict(frame)\n",
    "                            \n",
    "                            # 키포인트 시각화\n",
    "                            for name, point in person_keypoints.items():\n",
    "                                if name != 'bbox':  # 바운딩 박스 정보 제외\n",
    "                                    color = self.pose_predictor.keypoint_colors.get(name, (0, 255, 0))\n",
    "                                    cv2.circle(frame, tuple(point), 5, color, -1)\n",
    "\n",
    "                            # 스켈레톤 연결선 그리기\n",
    "                            for start, end in self.pose_predictor.skeleton_connections:\n",
    "                                if start in person_keypoints and end in person_keypoints:\n",
    "                                    start_point = tuple(person_keypoints[start])\n",
    "                                    end_point = tuple(person_keypoints[end])\n",
    "                                    \n",
    "                                    is_upper_body = all(point in ['left_shoulder', 'right_shoulder', \n",
    "                                                                'left_elbow', 'right_elbow',\n",
    "                                                                'left_wrist', 'right_wrist'] \n",
    "                                                    for point in [start, end])\n",
    "                                    color = (255, 0, 0) if is_upper_body else (0, 0, 255)\n",
    "                                    cv2.line(frame, start_point, end_point, color, 2)\n",
    "\n",
    "                            # 예측 결과 표시\n",
    "                            if 'bbox' in person_keypoints:\n",
    "                                x1, y1 = person_keypoints['bbox'][:2]\n",
    "                                cv2.putText(frame, str(pose_prediction), \n",
    "                                        (int(x1), int(y1) - 30),\n",
    "                                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"개별 사람 처리 중 오류 발생: {e}\")\n",
    "\n",
    "                # FPS 표시\n",
    "                current_time = time.time()\n",
    "                fps = 1.0 / (current_time - self.prev_time)\n",
    "                self.prev_time = current_time\n",
    "                cv2.putText(frame, f\"FPS: {fps:.1f}\", (10, frame.shape[0] - 20),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "            return frame\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"프레임 처리 중 오류 발생: {e}\")\n",
    "            return frame\n",
    "        \n",
    "    def draw_text_with_korean(self, img, text, position, font_color=(255, 255, 255)):\n",
    "        \"\"\"\n",
    "        한글 텍스트를 이미지에 그리는 메서드\n",
    "        \n",
    "        주요 기능:\n",
    "        - PIL을 사용한 한글 텍스트 렌더링\n",
    "        - OpenCV 이미지와 PIL 이미지 간 변환\n",
    "        - 사용자 지정 폰트 및 색상 지원\n",
    "        \n",
    "        Args:\n",
    "            img (np.ndarray): OpenCV 형식의 이미지\n",
    "            text (str): 그릴 텍스트 (한글 지원)\n",
    "            position (tuple): 텍스트를 그릴 위치 (x, y)\n",
    "            font_color (tuple): RGB 형식의 폰트 색상 (기본값: 흰색)\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: 텍스트가 그려진 이미지\n",
    "        \"\"\"\n",
    "        try:\n",
    "            img_pil = Image.fromarray(img)  # OpenCV -> PIL 변환\n",
    "            draw = ImageDraw.Draw(img_pil)  # 드로잉 객체 생성\n",
    "            draw.text(position, text, font=self.font, fill=font_color)\n",
    "            return np.array(img_pil)  # PIL -> OpenCV 변환\n",
    "        except Exception as e:\n",
    "            print(f\"텍스트 그리기 중 오류 발생: {e}\")\n",
    "            return img\n",
    "\n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"\n",
    "        프레임 처리를 위한 메인 인터페이스 메서드\n",
    "        \n",
    "        주요 기능:\n",
    "        - 프레임 스킵을 통한 성능 최적화\n",
    "        - FPS 측정 및 관리\n",
    "        - 프레임 처리 큐 관리\n",
    "        - 이전 프레임 캐싱\n",
    "        \n",
    "        처리 단계:\n",
    "        1. 프레임 스킵 로직 적용\n",
    "        2. FPS 업데이트\n",
    "        3. 프레임 처리 큐 관리\n",
    "        4. 결과 반환\n",
    "        \n",
    "        Args:\n",
    "            frame (np.ndarray): 처리할 입력 프레임\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: 처리된 프레임 또는 이전 프레임\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.frame_count += 1\n",
    "\n",
    "            # 프레임 스킵 로직\n",
    "            if self.frame_count % self.frame_skip != 0:\n",
    "                return self.previous_frame if self.previous_frame is not None else frame\n",
    "\n",
    "            # FPS 업데이트\n",
    "            current_time = time.time()\n",
    "            self.fps_deque.append(current_time - self.prev_time)\n",
    "            self.prev_time = current_time\n",
    "\n",
    "            # 입력 검증\n",
    "            if not isinstance(frame, np.ndarray):\n",
    "                frame = np.array(frame)\n",
    "\n",
    "            # 프레임 처리 큐에 추가\n",
    "            if not self.frame_queue.full():\n",
    "                self.frame_queue.put(frame)\n",
    "\n",
    "            # 처리된 결과 가져오기\n",
    "            if not self.result_queue.empty():\n",
    "                self.previous_frame = self.result_queue.get()\n",
    "                return self.previous_frame\n",
    "\n",
    "            return self.previous_frame if self.previous_frame is not None else frame\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"프레임 처리 중 오류 발생: {e}\")\n",
    "            return frame\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        실시간 포즈 감지 시스템의 메인 실행 루프\n",
    "        \n",
    "        주요 기능:\n",
    "        - 웹캠 초기화 및 설정\n",
    "        - 실시간 프레임 처리\n",
    "        - 사용자 입력 처리\n",
    "        - 리소스 정리\n",
    "        \n",
    "        실행 과정:\n",
    "        1. 웹캠 초기화 및 설정\n",
    "        2. 프레임 캡처 및 처리\n",
    "        3. 결과 표시\n",
    "        4. 종료 처리\n",
    "        \"\"\"\n",
    "        print(\"웹캠 초기화 중...\")\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        \n",
    "        # 웹캠 설정 (저해상도 설정으로 성능 최적화)\n",
    "        cap.set(cv2.CAP_PROP_FPS, 15)  # FPS 제한\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, self.frame_width)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, self.frame_height)\n",
    "        \n",
    "        print(\"웹캠 설정 완료\")\n",
    "        print(f\"해상도: {self.frame_width}x{self.frame_height}\")\n",
    "        print(f\"목표 FPS: 15\")\n",
    "\n",
    "        try:\n",
    "            while cap.isOpened() and self.running:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    print(\"카메라로부터 프레임을 가져오는데 실패했습니다.\")\n",
    "                    break\n",
    "\n",
    "                # 프레임 처리 로직\n",
    "                self.frame_count += 1\n",
    "                if self.frame_count % self.frame_skip != 0:\n",
    "                    continue\n",
    "\n",
    "                # 프레임 처리 큐 관리\n",
    "                if not self.frame_queue.full():\n",
    "                    self.frame_queue.put(frame)\n",
    "\n",
    "                # 결과 표시\n",
    "                if not self.result_queue.empty():\n",
    "                    processed_frame = self.result_queue.get()\n",
    "                    if processed_frame is not None:\n",
    "                        cv2.imshow('실시간 포즈 감지', processed_frame)\n",
    "\n",
    "                # 키 입력 처리\n",
    "                key = cv2.waitKey(1) & 0xFF\n",
    "                if key in [27, ord('q')]:  # ESC 또는 'q' 키\n",
    "                    break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"실행 중 오류 발생: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        finally:\n",
    "            # 리소스 정리\n",
    "            print(\"프로그램 종료 중...\")\n",
    "            self.running = False\n",
    "            if cap.isOpened():\n",
    "                cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    포즈 감지 시스템의 메인 실행 함수\n",
    "    \n",
    "    주요 기능:\n",
    "    - 시스템 환경 확인 및 초기화\n",
    "    - GPU/CPU 설정\n",
    "    - 모델 및 리소스 파일 경로 설정\n",
    "    - 실시간 포즈 감지 시스템 실행\n",
    "    \n",
    "    초기화 과정:\n",
    "    1. 시스템 정보 출력\n",
    "    2. GPU 사용 가능 여부 확인\n",
    "    3. 필요한 모델 및 리소스 파일 검증\n",
    "    4. 포즈 감지 시스템 실행\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 시스템 정보 출력\n",
    "        print(\"시스템 정보:\")\n",
    "        print(f\"Python 버전: {sys.version}\")\n",
    "        print(f\"OpenCV 버전: {cv2.__version__}\")\n",
    "        print(f\"PyTorch 버전: {torch.__version__}\")\n",
    "        print(f\"CPU 스레드 수: {os.cpu_count()}\")\n",
    "        \n",
    "        # GPU 사용 가능 여부 확인\n",
    "        if not torch.cuda.is_available():\n",
    "            print(\"GPU를 사용할 수 없어 CPU를 사용합니다\")\n",
    "            device = torch.device('cpu')\n",
    "        else:\n",
    "            print(\"GPU를 사용합니다\")\n",
    "            device = torch.device('cuda')\n",
    "\n",
    "        # 기본 디렉토리 설정\n",
    "        BASE_DIR = r\"C:\\coding\\python\\vs\\새 폴더\"\n",
    "        \n",
    "        # 모델 경로 설정\n",
    "        model_paths = {\n",
    "            'transformer': os.path.join(BASE_DIR, 'transformer_complete.pth'),\n",
    "            'mlp': os.path.join(BASE_DIR, 'mlp_complete.pth'),\n",
    "            'gru': os.path.join(BASE_DIR, 'gru_complete.pth'),\n",
    "            'cnn': os.path.join(BASE_DIR, 'cnn_complete.pth'),\n",
    "            'vit': os.path.join(BASE_DIR, 'vit_complete.pth'),\n",
    "            'stacking': os.path.join(BASE_DIR, 'stacking_complete.pth')\n",
    "        }\n",
    "\n",
    "        # YOLO 모델 및 폰트 파일 경로 설정\n",
    "        yolo_path = os.path.join(BASE_DIR, 'yolov8n.pt')        # 경량화된 모델\n",
    "        yolo_pose_path = os.path.join(BASE_DIR, 'yolov8x.pt')   # 포즈 추정 모델\n",
    "        font_path = r\"C:\\Users\\user\\Desktop\\vision\\GowunDodum-Regular.ttf\"\n",
    "\n",
    "        # 필요한 파일 존재 여부 확인\n",
    "        for name, path in model_paths.items():\n",
    "            if not os.path.exists(path):\n",
    "                print(f\"경고: {name} 모델 파일이 없습니다: {path}\")\n",
    "\n",
    "        # 필수 파일 검증\n",
    "        if not os.path.exists(yolo_path):\n",
    "            raise FileNotFoundError(f\"YOLO 모델 파일이 없습니다: {yolo_path}\")\n",
    "        \n",
    "        if not os.path.exists(yolo_pose_path):\n",
    "            raise FileNotFoundError(f\"YOLO Pose 모델 파일이 없습니다: {yolo_pose_path}\")\n",
    "            \n",
    "        if not os.path.exists(font_path):\n",
    "            raise FileNotFoundError(f\"폰트 파일이 없습니다: {font_path}\")\n",
    "\n",
    "        print(\"모든 필요 파일 확인 완료\")\n",
    "        print(\"프로그램 시작 중...\")\n",
    "\n",
    "        # PyTorch 성능 최적화 설정\n",
    "        torch.set_num_threads(4)  # CPU 스레드 수 제한\n",
    "        \n",
    "        # 실시간 포즈 예측기 초기화 및 실행\n",
    "        predictor = RealtimePosePredictor(model_paths, yolo_path, yolo_pose_path, font_path)\n",
    "        predictor.run()\n",
    "\n",
    "    except Exception as e:\n",
    "        # 예외 처리 및 상세 에러 로그 출력\n",
    "        print(f\"프로그램 실행 중 오류 발생: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    프로그램 시작점\n",
    "    \n",
    "    - 프로그램이 직접 실행될 때만 main() 함수 호출\n",
    "    - 모듈로 임포트될 때는 실행되지 않음\n",
    "    \"\"\"\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2203347687.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[18], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    py -3.10 -m pip install torch torchvision torchaudio\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "py -3.10 -m pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch.nn (from versions: none)\n",
      "ERROR: No matching distribution found for torch.nn\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\user\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install torch.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~pencv-python (c:\\Users\\82102\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "ERROR: Could not find a version that satisfies the requirement python (from versions: none)\n",
      "ERROR: No matching distribution found for python\n"
     ]
    }
   ],
   "source": [
    "pip install python\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
